@book{IEEE,
  title		= "Standards Coordinating Committee 10, Terms and Definitions. The IEEE Standard Dictionary of Electrical and Electronics Terms",
  author		= "J. Radatz, Ed",
  year		= "1996",
  publisher	= "IEEE"
}

@book{APC2011,
 author = {Gebali, Fayez},
 title = {Algorithms and Parallel Computing},
 year = {2011},
 isbn = {0470902108, 9780470902103},
 edition = {1st},
 publisher = {Wiley Publishing},
}

@book{Cormen94,
  title		= "Wprowadzenie do algorytmów",
  author		= "Cormen Thomas H. and Leiserson Charles E. and Rivest Roland L. and Stein Clifford",
  year		= "2001",
  publisher	= "WNT"
}

@book{JaJa92,
  title		= "Introduction to Parallel Algorithms",
  author		= "Joseph JaJa",
  year		= "1992",
  publisher	= "Addison-Wesley"
}

@Book{ Czech, author = "Czech, Zbigniew", editor = "", title = "Wprowadzenie do obliczeń równoległych", publisher = "Wydawnictwo Naukowe PWN", year = 2013, address = "Warszawa"} 

@article{Strassen68,
    author  = "Volker Strassen",
    title   = "Gaussian Elimination is not Optimal",
    year    = "1969",
    journal = "Numerische Mathematik",
    volume  = "13",
    pages   = "354--356"
}

@book{Stpiczyński,
	author = "Przemysław Stpiczyński and Marcin Brzuszek",
	title  = "Podstawy programowania obliczeń równoległych",
	year	   = "2011",
	publisher = "UMCS"
}

@Article{Winograd,
author = {Shmuel Winograd},
title = {On multiplication of \(2 \times 2\) matrices},
journal = {Linear Algebra and its Applications},
year = {1971},
volume = {4},
pages = {381-388}
}

@article{DBLP:journals/corr/abs-1202-3173,
  author    = {Grey Ballard and
               James Demmel and
               Olga Holtz and
               Benjamin Lipshitz and
               Oded Schwartz},
  title     = {Communication-Optimal Parallel Algorithm for Strassen's Matrix Multiplication},
  journal   = {CoRR},
  volume    = {abs/1202.3173},
  year      = {2012},
  url       = {http://arxiv.org/abs/1202.3173},
  timestamp = {Wed, 10 Oct 2012 21:28:48 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1202-3173},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Hedtke,
  added-at = {2011-12-05T00:00:00.000+0100},
  author = {Hedtke, Ivo},
  biburl = {http://www.bibsonomy.org/bibtex/2ba8b8f5f9dd29a471b189e92a0574e1c/dblp},
  ee = {http://arxiv.org/abs/1007.2117},
  interhash = {2672edb36acaeaad95a896db1fac6116},
  intrahash = {ba8b8f5f9dd29a471b189e92a0574e1c},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2011-12-06T12:42:51.000+0100},
  title = {Strassen's Matrix Multiplication Algorithm for Matrices of Arbitrary Order},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1007.html#abs-1007-2117},
  volume = {abs/1007.2117},
  year = 2010
}

@proceedings{Loeckx1974,
  editor    = {Jacques Loeckx},
  title     = {Automata, Languages and Programming, 2nd Colloquium, University
               of Saarbr{\"u}cken, July 29 - August 2, 1974, Proceedings},
  booktitle = {ICALP},
  publisher = {Springer},
  series    = {Lecture Notes in Computer Science},
  volume    = {14},
  year      = {1974},
  isbn      = {3-540-06841-4},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@techreport{communication_efficient,
          number = {1994-25},
          author = {H. Gupta and P. Sadayappan},
       booktitle = {Proceedings of the Sixth Annual ACM Symposium on Parallel Algorithms and Architectures, 1994 (SPAA '94)},
           title = {Communication Efficient Matrix-Multiplication on Hypercubes.},
            type = {Technical Report},
       publisher = {Stanford InfoLab},
     institution = {Stanford Infolab},
            year = {1994},
             url = {http://ilpubs.stanford.edu:8090/59/},
        abstract = {1 Introduction Dense matrix multiplication is used in a variety of applications and is one of the core components in many scientific computations. The standard way of multiplying two matrices of size n  n requires O(n 3 ) floating point operations on a sequential machine. Since dense matrix multiplication is computationally expensive, the development of effcient algorithms for large distributed memory machines is of great interest. Matrix multiplication is a very regular computation and lends itself well to parallel implementation. One of the effcient approaches to design other parallel matrix or graph algorithms is to decompose them into a sequence of matrix multiplications [3, 9]. One of the earliest distributed algorithms proposed for matrix multiplication was by Cannon [2] in 1969 for 2-D meshes. Ho, Johnsson, and Edelman in [8] presented a variant of Cannon's algorithm which uses the full bandwidth of a 2-D grid embedded in a hypercube. Some other algorithms are by Dekel, Nassimi, and Sahni [3], Berntsen [1] and Fox, Otto, and Hey [4]. Gupta and Kumar in [5] discuss the scalability of these algorithms and their variants. In this paper we propose two new algorithms for hypercubes. The algorithms proposed in this paper are better than all previously proposed algorithms for a wide range of matrix sizes and number of processors. The rest of the paper is organized as follows. In Section 2 we state our assumptions and discuss the communication models used. In Section 3 we discuss the previously proposed algorithms. In Section 4 we present the new algorithms. Section 5 presents some optimality results. In Section 6, we analyze the performance of the algorithms on hypercubes for three different communication cost W e present our conclusions in Section 7. 2 Communication Models In this paper we analyze the performance of the various algorithms presented for hypercube architectures. Throughout this paper, we refer to a 2-ary n-cube as a hypercube and all the logarithms}
}

@techreport{summa,
 author = {van de Geijn, Robert A. and Watts, Jerrell},
 title = {SUMMA: Scalable Universal Matrix Multiplication Algorithm},
 year = {1995},
 source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Autexas_cs%3AUTEXAS_CS%2F%2FCS-TR-95-13},
 publisher = {University of Texas at Austin},
 address = {Austin, TX, USA},
}

@techreport{Solomonik:EECS-2011-72,
    Author = {Solomonik, Edgar and Demmel, James},
    Title = {Communication-optimal parallel 2.5D matrix multiplication and LU factorization algorithms},
    Institution = {EECS Department, University of California, Berkeley},
    Year = {2011},
    Month = {Jun},
    URL = {http://www.eecs.berkeley.edu/Pubs/TechRpts/2011/EECS-2011-72.html},
    Number = {UCB/EECS-2011-72},
    Abstract = {Extra memory allows parallel matrix multiplication to be done with asymptotically less communication than Cannon’s algorithm and be faster in practice. “3D” algorithms arrange the p processors in a 3D array, and store redundant copies of the matrices on each of p^1/3 layers. ‘2D” algorithms such as Cannon’s algorithm store a single copy of the matrices on a 2D array of processors. We generalize these 2D and 3D algorithms by introducing a new class of “2.5D algorithms”. For matrix multiplication, we can take advantage of any amount of extra memory to store c copies of the data, for any c ∈ {1, 2, ..., p^1/3 }, to reduce the bandwidth cost of Cannon’s algorithm by a factor of c^1/2 and the latency cost by a factor c^3/2 . We also show that these costs reach the lower bounds, modulo polylog(p) factors. We introduce a novel algorithm for 2.5D LU decomposition. To the best of our knowledge, this LU algorithm is the first to minimize communication along the critical path of execution in the 3D case. Our 2.5D LU algorithm uses communication-avoiding pivoting, a stable alternative to partial-pivoting. We prove a novel lower bound on the latency cost of 2.5D and 3D LU factorization, showing that while c copies of the data can also reduce the bandwidth by a factor of c^1/2 , the latency must increase by a factor of c^1/2 , so that the 2D LU algorithm (c = 1) in fact minimizes latency. We provide implementations and performance results for 2D and 2.5D versions of all the new algorithms. Our results demonstrate that 2.5D matrix multiplication and LU algorithms strongly scale more efficiently than 2D algorithms. Each of our 2.5D algorithms performs over 2X faster than the corresponding 2D algorithm for certain problem sizes on 65,536 cores of a BG/P supercomputer.}
}

@phdthesis{Cannon:1969:CCI:905686,
 author = {Cannon, Lynn Elliot},
 title = {A Cellular Computer to Implement the Kalman Filter Algorithm},
 year = {1969},
 note = {AAI7010025},
 publisher = {Montana State University},
 address = {Bozeman, MT, USA},
}

@MISC{Douglas94gemmw,
    author = {Craig C. Douglas and Michael Heroux and Gordon Slishman and Roger M. Smith and Roger M},
    title = {Gemmw: A Portable Level 3 Blas Winograd Variant Of Strassen's Matrix-Matrix Multiply Algorithm},
    year = {1994}
}

@book{Quinn,
 author = {Quinn, Michael J.},
 title = {Parallel Programming in C with MPI and OpenMP},
 year = {2003},
 isbn = {0071232656},
 publisher = {McGraw-Hill Education Group},
} 